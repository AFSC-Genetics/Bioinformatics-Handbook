{"title":"SEDNA and Slurm","markdown":{"yaml":{"title":"SEDNA and Slurm","editor":"visual"},"headingText":"What is SEDNA and what is a cluster?","containsRefs":false,"markdown":"\n\n\nSEDNA is a computer cluster owned and operated by NOAA. It is a resource for folks working on bioinformatic work. A cluster computer is a set of computers that are interconnected like a network. They can be used as standalone computers to run simple jobs or can be interconnected to work together on computationally intensive tasks.\n\nAn individual computer is termed a 'node'. There are two different types of nodes: a **head node** where you land when you login to the cluster and **compute nodes** where you submit jobs. The **head node** is meant to view and edit directories or scripts and submit jobs. Any computing that needs to be done should ALWAYS be done on a **compute node** (more on this in User commands for Slurm). The compute nodes are managed by the scheduling software SLURM.\n\nTo set up an account on Sedna, you need to open a work request with the following [form](https://docs.google.com/forms/d/e/1FAIpQLSf2tDl9nJjihmHX9hM6ytMI3ToldqERVem1ge25-kp3JHw3tQ/viewform) and someone will contact you to set up your account. The current team managing Sedna is:\\\n- **Krista Nichols**, Genetics & Evolution Program Manager at the NWFSC krista.nichols\\@noaa.gov\\\n- **Giles Goetz**, Bioinformatics Specialist at the NWFSC giles.goetz\\@noaa.gov\\\n- **Marcus Nedelmann**, Linux System Administrator at the NWFSC marcus.nedelmann\\@noaa.gov\n\nTo access Sedna, you need to use the following command:\n\n```{python}\n#| eval: false\n    ssh <username>@sedna.nwfsc2.noaa.gov\n```\n\nor if this gives you issues you many need to use the direct IP address:\n\n```{python}\n#| eval: false\n    ssh <username>@161.55.52.157\n```\n\nOnce you are on Sedna you land in your user home directory `/home/<username>` where you have a default of 4 TB of space to perform your work on. Its typical to offload your project files to long term storage when it is completed to create more space on your partition. However, if you run out of space and can not clear off files because the projects are ongoing and you still need intermediate files, you can open a work request to increase your space allotment temporarily.\n\n### What is Slurm?\n\nSlurm is a cluster manager and job scheduling system for Linux based clusters. Its overarching main features are allocating access to resources to users for a given amount of time (with either defaulted time and amount of compute nodes or as specified by the user), provides a framework for starting, executing and monitoring jobs, and manages a queue of pending work.\n\n### Working on Sedna with Slurm\n\n#### Interactive shell\n\nAs stated above, when you login to the cluster you land on a **head node** and if you are doing anything other than moving through your directories and looking at files, it is best practice to move onto a **compute node** using an interactive shell. To do this, you should use the `srun` command which enables you to start an interactive session:\n\n```{python}\n#| eval: false\nsrun -c 1 -t 12:00:00 --pty /bin/bash\n```\n\nWhat this command is doing is creating what is called a pseudo-terminal `--pty` that gives you one compute node with the `-c 1` flag for 12 hours with the `-t 12:00:00` flag running bash `/bin/bash`. You should set the time for whatever you need. Its typical to request it for a full work day if you will be working on the cluster all day. After that time passes, the connection to the **compute node** will be terminated, but you can always just start another one with the same command if you need to continue work on it.\n\n#### Bash job\n\nThe above command is technically running a job on the cluster, but it is in an interactive format. It is good for moving files around, testing scripts that you are working on, zipping files, etc. Most scripts that you will run on the cluster will be non-interactive and run with a submission script using what is called a batch job. A batch job is run using the `sbatch` command on job files that end in `.sh`. The job file is the program that you wish to run but takes on a specific format. Here is an example header of a shell script:\n\n```{python}\n#| eval: false\n#!/bin/bash\n#SBATCH --job-name=pcod_peaks\n#SBATCH --time=24:00:00\n#SBATCH --mem=10GB\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov # update your email\n#SBATCH --output=../job_outfiles/highFSTpeaks.%j.out # update your out file directory\n#SBATCH --error=../job_outfiles/highFSTpeaks.%j.err # update your error readout directory\n\n```\n\nThe first line is what is called the \"shebang\" line and for a bash script is `#!/bin/bash`. What this line is doing is first telling unix you are about to tell it how to interpret the script with `#!` and then how to interpret the contents of the script with the `/bin/bash` (the path to bash on Sedna). All the following lines are Slurm options and must begin with `#SBATCH` to tell Slurm that these are the options for the Slurm job.\\\n`--job_name` sets the name of your job `--time` sets how long you want to use a compute node for this job in day-hours:mins:secs format; 1-12:00:00 would run for 1 and half days `--mem` sets the amount of RAM you need allocated to your job which can be set typically in either MB or GB of RAM. `--mail-type` tells Slurm when you want to be emailed about your job and in this case we want an email when the job fails with FAIL, but can be NONE (default), BEGIN, END, FAIL, REQUEUE, ALL. `--mail-user` sets the email you want Slurm to use `--output` sets the path and file name to put your stdout to go which will be any output that is given from the programs you are running in the job script `--error` sets the path and file name of the stderr which gives any issues that arose in the run. It is good practice to add the `%j` to these file names because it gives the filename the job ID from that run of the job script. This makes documenting and tracking different runs of the same script easy.\n\nOther common options include `--cpus-per-task=<ncpus>` where ncpus is the number of cores you need to run your job and is used when your job can `partition=<partitionName>` where partitionName is the name of the partition you want your job run on. The latter option is used on Sedna when for example you want your job to run on the himem computers. These are only for jobs that require a large amount of RAM and thus the default on Slurm is to run on the standard partition with lower RAM capacity computers. There will be instances however where you exceed the RAM on those (see below section on Sedna hardware).\n\nAny additional lines in the job script will be lines running the specific program/command you need to run. Here is a simple example in a file called `echoTest.sh`:\n\n```{python}\n#| eval: false\n#!/bin/bash\n#SBATCH --job-name=echo_test\n#SBATCH --time=00:05:00\n#SBATCH --mem=10MB\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov \n#SBATCH --output=../job_outfiles/echo_test.%j.out \n#SBATCH --error=../job_outfiles/echo_test.%j.err\n\n\necho \"Today is $(date)\"\n```\n\nThis script prints the date to your stdout file using the `echo` command. Once you set up your job options and have the program set in the script to run you would run the script from the folder that script is in using `sbatch echoTest.sh`. When you run that line, Slurm should output the job ID that it gave your job:\n\n```{python}\n#| eval: false\n> Submitted batch job 696063\n```\n\n#### Other useful commands in Slurm\n\nYou can view the jobs that are running `squeue` on the command line. This will give you an output of all the current jobs running on Sedna:\n\n```{python}\n#| eval: false\n JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n693037      node snake_ma    sgurr  R 2-23:43:37      1 node03\n695733      node snakejob    sgurr  R 1-12:22:32      1 node29\n696063\t\tnode echo_test sschaal  R   00:00:01      1 node19\n\n```\n\n`JOBID` are the IDs of the current jobs that are running `PARTITION` is the parition that the jobs are running on which will be node for standard nodes and himem for the high memory nodes `NAME` is the name you gave your job `USER` is the user that is running the job `ST` is the state the job is in which is R for running, PD for pending, CG for completing, CD for completed, F for failed. The latter three are only briefly printed out in the squeue output. Once they reach those points the job is removed from the squeue. `TIME` is how long the job has been running `NODES` are the number of nodes the job is running on `NODELIST` are the names of the nodes that the job is running on or if the job is not running yet it will give a reason that it isn't running\n\nIf you just want to see your jobs that are running you can add the user flag when running this command `squeue -u <username>` and then you will only see a list of your jobs.\n\nIf your job is in a pending state, that may mean that there are not resources available at the moment to run your job. You can check the status of the cluster nodes by running `sinfo` which will give an output like the example output below:\n\n```{python}\n#| eval: false\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnode*        up   infinite      2    mix node[03,29]\nnode*        up   infinite     10  alloc node[07,20-21,30-36]\nnode*        up   infinite     24   idle node[01-02,04-06,08-19,22-28]\nhimem        up   infinite      4   idle himem[01-04]\nbionode      up   infinite      1 drain* bionode12\nbionode      up   infinite     18  down* bionode[01-11,13-19]\n\n```\n\n`PARTITION` which partition the line refers to which will be either node for standard nodes, himem for the high memory nodes, or bionode which are depreciated nodes that we no longer can access, but are still always listed on the sinfo\\\n`AVAIL` the active state of the partition either up, down or idle\\\n`TIMELIMIT` the maximum job execution walltime per partition in our case they have no limit\\\n`NODES` the total number of nodes per parition per state\\\n`STATE` the current state of the nodes which is either mix, alloc, idle, drain, or down. If in a mix state that means only part of the available RAM is being used by the node and can be allocated to another job. If in an alloc state, that means the whole node is allocated to a job(s). If in an idle state, these nodes are not active and available to use. If in a drain or down state, these nodes are under maintenance or depreciated.\\\n`NODELIST` the partition name and the node numbers in each given state\n\nIf you start a job, but realize something is not correct and you need to cancel the job you can cancel the job in a few different ways. The first is if it is a single job you can use the job ID `scancel <jobID>` or if you only have a single job running and you do not have the job ID handy you can also use `scancel -u <username>`. If you have multiple jobs running and you want to cancel them all the previous command will also work. You can also cancel a range of job IDs `scancel {<firstJobID>..<lastJobID>}`.\n\nAfter you complete a job, you can check how the efficency of your runs cpu and memory usage using the `seff <jobID>` command. This command will give you some important information about your run especially if you are in the process of testing scripts for how much memory or time you need to allocate to the job. An example output is below:\n\n```{python}\n#| eval: false\nJob ID: 163074\nCluster: sedna\nUser/Group: sschaal/sschaal\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 1-09:11:35\nCPU Efficiency: 15.34% of 9-00:26:24 core-walltime\nJob Wall-clock time: 13:31:39\nMemory Utilized: 4.43 GB\nMemory Efficiency: 44.28% of 10.00 GB\n\n```\n\n`CPU Utilized` sum of the computer time the job took to run. In this example, this job was run on 16 cores of a node, and so the CPU time is the sum of the time that was used on each core. `CPU Efficiency` shows how efficient the cpu use was and in this case it was low because some cores were not utilized for the entire job run. This can be optimized by reducing the number of cores needed for the run. `Job Wall-clock time` actual time the job took to run `Memory Utilized` maximum amount of RAM that was used during the run. `Memory Efficiency` tells you what percentage of the memory that you allocated to the job was actually used. In this example, the job only used roughly half of the memory that was allocated to it and therefore, could be run using 5 or 6MB of RAM in future. It is always good to give it a little bit higher than what you need just in case.\n\n### Software on Sedna\n\nSedna uses modules in order to provide individuals with access to different software that has been install. To check what software is currently available on Sedna type `module avail`. For bioinformatics work, the majority of software will be listed under\n\nThe first time you want to access modules on the cluster you need to type the following:\n\n```{python}\n#| eval: false\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' >> ~/.bashrc\n```\n\nThen logout of the cluster and log back in. This will set you up to use all the modules available to the cluster. If there is not a program listed that you need access to then open a work requesst with the Sedna staff at the following [link](https://docs.google.com/forms/d/e/1FAIpQLSf2tDl9nJjihmHX9hM6ytMI3ToldqERVem1ge25-kp3JHw3tQ/viewform).\n\n### Sedna hardware\n\nThere are three types of nodes on Sedna. The **Standard compute nodes** have two different memory features. Standard compute node01 to node28 have 96GB of RAM whereas node29 to node36 have 192 GB of RAM. If your job is in a pending status, but there are available nodes, it could be because you requested more than 96 GB of RAM and node29 to node36 are currently being used. The other available nodes are the four **himem** nodes himem01 to himem04 that have 1.5 TB of RAM and should only be used for jobs requiring more than 192 GB of RAM.\n\n### An example bioinformatics job\n\nLets say that we want to index a genome for one of our bioinformatics projects. Here is an example batch job that would do this for the Atlantic cod gadMor3.0 genome:\n\n```{python}\n#| eval: false\n#!/bin/bash\n#SBATCH --mem=1GB\n#SBATCH --time=1:00:00\n#SBATCH --job-name=bwa_index_GCF_902167405.1_gadMor3.0_genomic\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov \n#SBATCH --output=../job_outfiles/bwa-index_GCF_902167405.1_gadMor3.0_genomic.%j.out\n#SBATCH --error=../job_outfiles/bwa-index_GCF_902167405.1_gadMor3.0_genomic.%j.err\n\nmodule unload aligners/bwa/0.7.17\nmodule load aligners/bwa/0.7.17\n\nbwa index -p /home/sschaal/pcod/20220125/novaseq/bwa/GCF_902167405.1_gadMor3.0_genomic /home/sschaal/ref_genomes/GCF_902167405.1_gadMor3.0_genomic.fna\n\n```\n\nIn this example, 1GB of RAM was allocated for 1 hour. The stdout and stderr went to our job_outfiles directory and was given the jobID in the file name. If the job failed and email would be sent to the given email. In order to run the index, the program *bwa* was needed and therefore, needed to be loaded via a module. As a refresher, you can get the name of the module and the path to the program in that module using the `module avail` command. The last line is *bwa index* script for indexing the reference genome.\n","srcMarkdownNoYaml":"\n\n### What is SEDNA and what is a cluster?\n\nSEDNA is a computer cluster owned and operated by NOAA. It is a resource for folks working on bioinformatic work. A cluster computer is a set of computers that are interconnected like a network. They can be used as standalone computers to run simple jobs or can be interconnected to work together on computationally intensive tasks.\n\nAn individual computer is termed a 'node'. There are two different types of nodes: a **head node** where you land when you login to the cluster and **compute nodes** where you submit jobs. The **head node** is meant to view and edit directories or scripts and submit jobs. Any computing that needs to be done should ALWAYS be done on a **compute node** (more on this in User commands for Slurm). The compute nodes are managed by the scheduling software SLURM.\n\nTo set up an account on Sedna, you need to open a work request with the following [form](https://docs.google.com/forms/d/e/1FAIpQLSf2tDl9nJjihmHX9hM6ytMI3ToldqERVem1ge25-kp3JHw3tQ/viewform) and someone will contact you to set up your account. The current team managing Sedna is:\\\n- **Krista Nichols**, Genetics & Evolution Program Manager at the NWFSC krista.nichols\\@noaa.gov\\\n- **Giles Goetz**, Bioinformatics Specialist at the NWFSC giles.goetz\\@noaa.gov\\\n- **Marcus Nedelmann**, Linux System Administrator at the NWFSC marcus.nedelmann\\@noaa.gov\n\nTo access Sedna, you need to use the following command:\n\n```{python}\n#| eval: false\n    ssh <username>@sedna.nwfsc2.noaa.gov\n```\n\nor if this gives you issues you many need to use the direct IP address:\n\n```{python}\n#| eval: false\n    ssh <username>@161.55.52.157\n```\n\nOnce you are on Sedna you land in your user home directory `/home/<username>` where you have a default of 4 TB of space to perform your work on. Its typical to offload your project files to long term storage when it is completed to create more space on your partition. However, if you run out of space and can not clear off files because the projects are ongoing and you still need intermediate files, you can open a work request to increase your space allotment temporarily.\n\n### What is Slurm?\n\nSlurm is a cluster manager and job scheduling system for Linux based clusters. Its overarching main features are allocating access to resources to users for a given amount of time (with either defaulted time and amount of compute nodes or as specified by the user), provides a framework for starting, executing and monitoring jobs, and manages a queue of pending work.\n\n### Working on Sedna with Slurm\n\n#### Interactive shell\n\nAs stated above, when you login to the cluster you land on a **head node** and if you are doing anything other than moving through your directories and looking at files, it is best practice to move onto a **compute node** using an interactive shell. To do this, you should use the `srun` command which enables you to start an interactive session:\n\n```{python}\n#| eval: false\nsrun -c 1 -t 12:00:00 --pty /bin/bash\n```\n\nWhat this command is doing is creating what is called a pseudo-terminal `--pty` that gives you one compute node with the `-c 1` flag for 12 hours with the `-t 12:00:00` flag running bash `/bin/bash`. You should set the time for whatever you need. Its typical to request it for a full work day if you will be working on the cluster all day. After that time passes, the connection to the **compute node** will be terminated, but you can always just start another one with the same command if you need to continue work on it.\n\n#### Bash job\n\nThe above command is technically running a job on the cluster, but it is in an interactive format. It is good for moving files around, testing scripts that you are working on, zipping files, etc. Most scripts that you will run on the cluster will be non-interactive and run with a submission script using what is called a batch job. A batch job is run using the `sbatch` command on job files that end in `.sh`. The job file is the program that you wish to run but takes on a specific format. Here is an example header of a shell script:\n\n```{python}\n#| eval: false\n#!/bin/bash\n#SBATCH --job-name=pcod_peaks\n#SBATCH --time=24:00:00\n#SBATCH --mem=10GB\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov # update your email\n#SBATCH --output=../job_outfiles/highFSTpeaks.%j.out # update your out file directory\n#SBATCH --error=../job_outfiles/highFSTpeaks.%j.err # update your error readout directory\n\n```\n\nThe first line is what is called the \"shebang\" line and for a bash script is `#!/bin/bash`. What this line is doing is first telling unix you are about to tell it how to interpret the script with `#!` and then how to interpret the contents of the script with the `/bin/bash` (the path to bash on Sedna). All the following lines are Slurm options and must begin with `#SBATCH` to tell Slurm that these are the options for the Slurm job.\\\n`--job_name` sets the name of your job `--time` sets how long you want to use a compute node for this job in day-hours:mins:secs format; 1-12:00:00 would run for 1 and half days `--mem` sets the amount of RAM you need allocated to your job which can be set typically in either MB or GB of RAM. `--mail-type` tells Slurm when you want to be emailed about your job and in this case we want an email when the job fails with FAIL, but can be NONE (default), BEGIN, END, FAIL, REQUEUE, ALL. `--mail-user` sets the email you want Slurm to use `--output` sets the path and file name to put your stdout to go which will be any output that is given from the programs you are running in the job script `--error` sets the path and file name of the stderr which gives any issues that arose in the run. It is good practice to add the `%j` to these file names because it gives the filename the job ID from that run of the job script. This makes documenting and tracking different runs of the same script easy.\n\nOther common options include `--cpus-per-task=<ncpus>` where ncpus is the number of cores you need to run your job and is used when your job can `partition=<partitionName>` where partitionName is the name of the partition you want your job run on. The latter option is used on Sedna when for example you want your job to run on the himem computers. These are only for jobs that require a large amount of RAM and thus the default on Slurm is to run on the standard partition with lower RAM capacity computers. There will be instances however where you exceed the RAM on those (see below section on Sedna hardware).\n\nAny additional lines in the job script will be lines running the specific program/command you need to run. Here is a simple example in a file called `echoTest.sh`:\n\n```{python}\n#| eval: false\n#!/bin/bash\n#SBATCH --job-name=echo_test\n#SBATCH --time=00:05:00\n#SBATCH --mem=10MB\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov \n#SBATCH --output=../job_outfiles/echo_test.%j.out \n#SBATCH --error=../job_outfiles/echo_test.%j.err\n\n\necho \"Today is $(date)\"\n```\n\nThis script prints the date to your stdout file using the `echo` command. Once you set up your job options and have the program set in the script to run you would run the script from the folder that script is in using `sbatch echoTest.sh`. When you run that line, Slurm should output the job ID that it gave your job:\n\n```{python}\n#| eval: false\n> Submitted batch job 696063\n```\n\n#### Other useful commands in Slurm\n\nYou can view the jobs that are running `squeue` on the command line. This will give you an output of all the current jobs running on Sedna:\n\n```{python}\n#| eval: false\n JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n693037      node snake_ma    sgurr  R 2-23:43:37      1 node03\n695733      node snakejob    sgurr  R 1-12:22:32      1 node29\n696063\t\tnode echo_test sschaal  R   00:00:01      1 node19\n\n```\n\n`JOBID` are the IDs of the current jobs that are running `PARTITION` is the parition that the jobs are running on which will be node for standard nodes and himem for the high memory nodes `NAME` is the name you gave your job `USER` is the user that is running the job `ST` is the state the job is in which is R for running, PD for pending, CG for completing, CD for completed, F for failed. The latter three are only briefly printed out in the squeue output. Once they reach those points the job is removed from the squeue. `TIME` is how long the job has been running `NODES` are the number of nodes the job is running on `NODELIST` are the names of the nodes that the job is running on or if the job is not running yet it will give a reason that it isn't running\n\nIf you just want to see your jobs that are running you can add the user flag when running this command `squeue -u <username>` and then you will only see a list of your jobs.\n\nIf your job is in a pending state, that may mean that there are not resources available at the moment to run your job. You can check the status of the cluster nodes by running `sinfo` which will give an output like the example output below:\n\n```{python}\n#| eval: false\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnode*        up   infinite      2    mix node[03,29]\nnode*        up   infinite     10  alloc node[07,20-21,30-36]\nnode*        up   infinite     24   idle node[01-02,04-06,08-19,22-28]\nhimem        up   infinite      4   idle himem[01-04]\nbionode      up   infinite      1 drain* bionode12\nbionode      up   infinite     18  down* bionode[01-11,13-19]\n\n```\n\n`PARTITION` which partition the line refers to which will be either node for standard nodes, himem for the high memory nodes, or bionode which are depreciated nodes that we no longer can access, but are still always listed on the sinfo\\\n`AVAIL` the active state of the partition either up, down or idle\\\n`TIMELIMIT` the maximum job execution walltime per partition in our case they have no limit\\\n`NODES` the total number of nodes per parition per state\\\n`STATE` the current state of the nodes which is either mix, alloc, idle, drain, or down. If in a mix state that means only part of the available RAM is being used by the node and can be allocated to another job. If in an alloc state, that means the whole node is allocated to a job(s). If in an idle state, these nodes are not active and available to use. If in a drain or down state, these nodes are under maintenance or depreciated.\\\n`NODELIST` the partition name and the node numbers in each given state\n\nIf you start a job, but realize something is not correct and you need to cancel the job you can cancel the job in a few different ways. The first is if it is a single job you can use the job ID `scancel <jobID>` or if you only have a single job running and you do not have the job ID handy you can also use `scancel -u <username>`. If you have multiple jobs running and you want to cancel them all the previous command will also work. You can also cancel a range of job IDs `scancel {<firstJobID>..<lastJobID>}`.\n\nAfter you complete a job, you can check how the efficency of your runs cpu and memory usage using the `seff <jobID>` command. This command will give you some important information about your run especially if you are in the process of testing scripts for how much memory or time you need to allocate to the job. An example output is below:\n\n```{python}\n#| eval: false\nJob ID: 163074\nCluster: sedna\nUser/Group: sschaal/sschaal\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 1-09:11:35\nCPU Efficiency: 15.34% of 9-00:26:24 core-walltime\nJob Wall-clock time: 13:31:39\nMemory Utilized: 4.43 GB\nMemory Efficiency: 44.28% of 10.00 GB\n\n```\n\n`CPU Utilized` sum of the computer time the job took to run. In this example, this job was run on 16 cores of a node, and so the CPU time is the sum of the time that was used on each core. `CPU Efficiency` shows how efficient the cpu use was and in this case it was low because some cores were not utilized for the entire job run. This can be optimized by reducing the number of cores needed for the run. `Job Wall-clock time` actual time the job took to run `Memory Utilized` maximum amount of RAM that was used during the run. `Memory Efficiency` tells you what percentage of the memory that you allocated to the job was actually used. In this example, the job only used roughly half of the memory that was allocated to it and therefore, could be run using 5 or 6MB of RAM in future. It is always good to give it a little bit higher than what you need just in case.\n\n### Software on Sedna\n\nSedna uses modules in order to provide individuals with access to different software that has been install. To check what software is currently available on Sedna type `module avail`. For bioinformatics work, the majority of software will be listed under\n\nThe first time you want to access modules on the cluster you need to type the following:\n\n```{python}\n#| eval: false\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' >> ~/.bashrc\n```\n\nThen logout of the cluster and log back in. This will set you up to use all the modules available to the cluster. If there is not a program listed that you need access to then open a work requesst with the Sedna staff at the following [link](https://docs.google.com/forms/d/e/1FAIpQLSf2tDl9nJjihmHX9hM6ytMI3ToldqERVem1ge25-kp3JHw3tQ/viewform).\n\n### Sedna hardware\n\nThere are three types of nodes on Sedna. The **Standard compute nodes** have two different memory features. Standard compute node01 to node28 have 96GB of RAM whereas node29 to node36 have 192 GB of RAM. If your job is in a pending status, but there are available nodes, it could be because you requested more than 96 GB of RAM and node29 to node36 are currently being used. The other available nodes are the four **himem** nodes himem01 to himem04 that have 1.5 TB of RAM and should only be used for jobs requiring more than 192 GB of RAM.\n\n### An example bioinformatics job\n\nLets say that we want to index a genome for one of our bioinformatics projects. Here is an example batch job that would do this for the Atlantic cod gadMor3.0 genome:\n\n```{python}\n#| eval: false\n#!/bin/bash\n#SBATCH --mem=1GB\n#SBATCH --time=1:00:00\n#SBATCH --job-name=bwa_index_GCF_902167405.1_gadMor3.0_genomic\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov \n#SBATCH --output=../job_outfiles/bwa-index_GCF_902167405.1_gadMor3.0_genomic.%j.out\n#SBATCH --error=../job_outfiles/bwa-index_GCF_902167405.1_gadMor3.0_genomic.%j.err\n\nmodule unload aligners/bwa/0.7.17\nmodule load aligners/bwa/0.7.17\n\nbwa index -p /home/sschaal/pcod/20220125/novaseq/bwa/GCF_902167405.1_gadMor3.0_genomic /home/sschaal/ref_genomes/GCF_902167405.1_gadMor3.0_genomic.fna\n\n```\n\nIn this example, 1GB of RAM was allocated for 1 hour. The stdout and stderr went to our job_outfiles directory and was given the jobID in the file name. If the job failed and email would be sent to the given email. In order to run the index, the program *bwa* was needed and therefore, needed to be loaded via a module. As a refresher, you can get the name of the module and the path to the program in that module using the `module avail` command. The last line is *bwa index* script for indexing the reference genome.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":true,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"output-file":"01-SEDNA_Slurm.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.541","bibliography":["references.bib","book.bib"],"title":"SEDNA and Slurm","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}