[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AFSC Bioinformatics Handbook",
    "section": "",
    "text": "Welcome to the AFSC Bioinformatics Handbook. This handbook is meant to be a guide for new and current lab members doing any bioinformatic work. The book covers creating an account with SEDNA, using the Slurm submission scheduler, local and command line text editors, basic awk/sed/grep commands, …\nIf there is additional information that you would like to see on this page please reach out to Sara Schaal (sara.schaal@noaa.gov) or Patrick Barry (patrick.barry@noaa.gov).\nAdditional bioinformatics resources:\n\nEric Anderson Bioinformatics Guide\nSEDNA Guide",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "01-SEDNA_Slurm.html",
    "href": "01-SEDNA_Slurm.html",
    "title": "2  SEDNA and Slurm",
    "section": "",
    "text": "2.0.1 What is SEDNA and what is a cluster?\nSEDNA is a computer cluster owned and operated by NOAA. It is a resource for folks working on bioinformatic work. A cluster computer is a set of computers that are interconnected like a network. They can be used as standalone computers to run simple jobs or can be interconnected to work together on computationally intensive tasks.\nAn individual computer is termed a ‘node’. There are two different types of nodes: a head node where you land when you login to the cluster and compute nodes where you submit jobs. The head node is meant to view and edit directories or scripts and submit jobs. Any computing that needs to be done should ALWAYS be done on a compute node (more on this in User commands for Slurm). The compute nodes are managed by the scheduling software SLURM.\nTo set up an account on Sedna, you need to open a work request with the following form and someone will contact you to set up your account. The current team managing Sedna is:\n- Krista Nichols, Genetics & Evolution Program Manager at the NWFSC krista.nichols@noaa.gov\n- Giles Goetz, Bioinformatics Specialist at the NWFSC giles.goetz@noaa.gov\n- Marcus Nedelmann, Linux System Administrator at the NWFSC marcus.nedelmann@noaa.gov\nTo access Sedna, you need to use the following command:\n\n    ssh &lt;username&gt;@sedna.nwfsc2.noaa.gov\n\nor if this gives you issues you many need to use the direct IP address:\n\n    ssh &lt;username&gt;@161.55.52.157\n\nOnce you are on Sedna you land in your user home directory /home/&lt;username&gt; where you have a default of 4 TB of space to perform your work on. Its typical to offload your project files to long term storage when it is completed to create more space on your partition. However, if you run out of space and can not clear off files because the projects are ongoing and you still need intermediate files, you can open a work request to increase your space allotment temporarily.\n\n\n2.0.2 What is Slurm?\nSlurm is a cluster manager and job scheduling system for Linux based clusters. Its overarching main features are allocating access to resources to users for a given amount of time (with either defaulted time and amount of compute nodes or as specified by the user), provides a framework for starting, executing and monitoring jobs, and manages a queue of pending work.\n\n\n2.0.3 Working on Sedna with Slurm\n\n2.0.3.1 Interactive shell\nAs stated above, when you login to the cluster you land on a head node and if you are doing anything other than moving through your directories and looking at files, it is best practice to move onto a compute node using an interactive shell. To do this, you should use the srun command which enables you to start an interactive session:\n\nsrun -c 1 -t 12:00:00 --pty /bin/bash\n\nWhat this command is doing is creating what is called a pseudo-terminal --pty that gives you one compute node with the -c 1 flag for 12 hours with the -t 12:00:00 flag running bash /bin/bash. You should set the time for whatever you need. Its typical to request it for a full work day if you will be working on the cluster all day. After that time passes, the connection to the compute node will be terminated, but you can always just start another one with the same command if you need to continue work on it.\n\n\n2.0.3.2 Bash job\nThe above command is technically running a job on the cluster, but it is in an interactive format. It is good for moving files around, testing scripts that you are working on, zipping files, etc. Most scripts that you will run on the cluster will be non-interactive and run with a submission script using what is called a batch job. A batch job is run using the sbatch command on job files that end in .sh. The job file is the program that you wish to run but takes on a specific format. Here is an example header of a shell script:\n\n#!/bin/bash\n#SBATCH --job-name=pcod_peaks\n#SBATCH --time=24:00:00\n#SBATCH --mem=10GB\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov # update your email\n#SBATCH --output=../job_outfiles/highFSTpeaks.%j.out # update your out file directory\n#SBATCH --error=../job_outfiles/highFSTpeaks.%j.err # update your error readout directory\n\nThe first line is what is called the “shebang” line and for a bash script is #!/bin/bash. What this line is doing is first telling unix you are about to tell it how to interpret the script with #! and then how to interpret the contents of the script with the /bin/bash (the path to bash on Sedna). All the following lines are Slurm options and must begin with #SBATCH to tell Slurm that these are the options for the Slurm job.\n--job_name sets the name of your job --time sets how long you want to use a compute node for this job in day-hours:mins:secs format; 1-12:00:00 would run for 1 and half days --mem sets the amount of RAM you need allocated to your job which can be set typically in either MB or GB of RAM. --mail-type tells Slurm when you want to be emailed about your job and in this case we want an email when the job fails with FAIL, but can be NONE (default), BEGIN, END, FAIL, REQUEUE, ALL. --mail-user sets the email you want Slurm to use --output sets the path and file name to put your stdout to go which will be any output that is given from the programs you are running in the job script --error sets the path and file name of the stderr which gives any issues that arose in the run. It is good practice to add the %j to these file names because it gives the filename the job ID from that run of the job script. This makes documenting and tracking different runs of the same script easy.\nOther common options include --cpus-per-task=&lt;ncpus&gt; where ncpus is the number of cores you need to run your job and is used when your job can partition=&lt;partitionName&gt; where partitionName is the name of the partition you want your job run on. The latter option is used on Sedna when for example you want your job to run on the himem computers. These are only for jobs that require a large amount of RAM and thus the default on Slurm is to run on the standard partition with lower RAM capacity computers. There will be instances however where you exceed the RAM on those (see below section on Sedna hardware).\nAny additional lines in the job script will be lines running the specific program/command you need to run. Here is a simple example in a file called echoTest.sh:\n\n#!/bin/bash\n#SBATCH --job-name=echo_test\n#SBATCH --time=00:05:00\n#SBATCH --mem=10MB\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov \n#SBATCH --output=../job_outfiles/echo_test.%j.out \n#SBATCH --error=../job_outfiles/echo_test.%j.err\n\n\necho \"Today is $(date)\"\n\nThis script prints the date to your stdout file using the echo command. Once you set up your job options and have the program set in the script to run you would run the script from the folder that script is in using sbatch echoTest.sh. When you run that line, Slurm should output the job ID that it gave your job:\n\n&gt; Submitted batch job 696063\n\n\n\n2.0.3.3 Other useful commands in Slurm\nYou can view the jobs that are running squeue on the command line. This will give you an output of all the current jobs running on Sedna:\n\n JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n693037      node snake_ma    sgurr  R 2-23:43:37      1 node03\n695733      node snakejob    sgurr  R 1-12:22:32      1 node29\n696063      node echo_test sschaal  R   00:00:01      1 node19\n\nJOBID are the IDs of the current jobs that are running PARTITION is the parition that the jobs are running on which will be node for standard nodes and himem for the high memory nodes NAME is the name you gave your job USER is the user that is running the job ST is the state the job is in which is R for running, PD for pending, CG for completing, CD for completed, F for failed. The latter three are only briefly printed out in the squeue output. Once they reach those points the job is removed from the squeue. TIME is how long the job has been running NODES are the number of nodes the job is running on NODELIST are the names of the nodes that the job is running on or if the job is not running yet it will give a reason that it isn’t running\nIf you just want to see your jobs that are running you can add the user flag when running this command squeue -u &lt;username&gt; and then you will only see a list of your jobs.\nIf your job is in a pending state, that may mean that there are not resources available at the moment to run your job. You can check the status of the cluster nodes by running sinfo which will give an output like the example output below:\n\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nnode*        up   infinite      2    mix node[03,29]\nnode*        up   infinite     10  alloc node[07,20-21,30-36]\nnode*        up   infinite     24   idle node[01-02,04-06,08-19,22-28]\nhimem        up   infinite      4   idle himem[01-04]\nbionode      up   infinite      1 drain* bionode12\nbionode      up   infinite     18  down* bionode[01-11,13-19]\n\nPARTITION which partition the line refers to which will be either node for standard nodes, himem for the high memory nodes, or bionode which are depreciated nodes that we no longer can access, but are still always listed on the sinfo\nAVAIL the active state of the partition either up, down or idle\nTIMELIMIT the maximum job execution walltime per partition in our case they have no limit\nNODES the total number of nodes per parition per state\nSTATE the current state of the nodes which is either mix, alloc, idle, drain, or down. If in a mix state that means only part of the available RAM is being used by the node and can be allocated to another job. If in an alloc state, that means the whole node is allocated to a job(s). If in an idle state, these nodes are not active and available to use. If in a drain or down state, these nodes are under maintenance or depreciated.\nNODELIST the partition name and the node numbers in each given state\nIf you start a job, but realize something is not correct and you need to cancel the job you can cancel the job in a few different ways. The first is if it is a single job you can use the job ID scancel &lt;jobID&gt; or if you only have a single job running and you do not have the job ID handy you can also use scancel -u &lt;username&gt;. If you have multiple jobs running and you want to cancel them all the previous command will also work. You can also cancel a range of job IDs scancel {&lt;firstJobID&gt;..&lt;lastJobID&gt;}.\nAfter you complete a job, you can check how the efficency of your runs cpu and memory usage using the seff &lt;jobID&gt; command. This command will give you some important information about your run especially if you are in the process of testing scripts for how much memory or time you need to allocate to the job. An example output is below:\n\nJob ID: 163074\nCluster: sedna\nUser/Group: sschaal/sschaal\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 1-09:11:35\nCPU Efficiency: 15.34% of 9-00:26:24 core-walltime\nJob Wall-clock time: 13:31:39\nMemory Utilized: 4.43 GB\nMemory Efficiency: 44.28% of 10.00 GB\n\nCPU Utilized sum of the computer time the job took to run. In this example, this job was run on 16 cores of a node, and so the CPU time is the sum of the time that was used on each core. CPU Efficiency shows how efficient the cpu use was and in this case it was low because some cores were not utilized for the entire job run. This can be optimized by reducing the number of cores needed for the run. Job Wall-clock time actual time the job took to run Memory Utilized maximum amount of RAM that was used during the run. Memory Efficiency tells you what percentage of the memory that you allocated to the job was actually used. In this example, the job only used roughly half of the memory that was allocated to it and therefore, could be run using 5 or 6MB of RAM in future. It is always good to give it a little bit higher than what you need just in case.\n\n\n\n2.0.4 Software on Sedna\nSedna uses modules in order to provide individuals with access to different software that has been install. To check what software is currently available on Sedna type module avail. For bioinformatics work, the majority of software will be listed under\nThe first time you want to access modules on the cluster you need to type the following:\n\necho 'export MODULEPATH=${MODULEPATH}:/opt/bioinformatics/modulefiles' &gt;&gt; ~/.bashrc\n\nThen logout of the cluster and log back in. This will set you up to use all the modules available to the cluster. If there is not a program listed that you need access to then open a work requesst with the Sedna staff at the following link.\n\n\n2.0.5 Sedna hardware\nThere are three types of nodes on Sedna. The Standard compute nodes have two different memory features. Standard compute node01 to node28 have 96GB of RAM whereas node29 to node36 have 192 GB of RAM. If your job is in a pending status, but there are available nodes, it could be because you requested more than 96 GB of RAM and node29 to node36 are currently being used. The other available nodes are the four himem nodes himem01 to himem04 that have 1.5 TB of RAM and should only be used for jobs requiring more than 192 GB of RAM.\n\n\n2.0.6 An example bioinformatics job\nLets say that we want to index a genome for one of our bioinformatics projects. Here is an example batch job that would do this for the Atlantic cod gadMor3.0 genome:\n\n#!/bin/bash\n#SBATCH --mem=1GB\n#SBATCH --time=1:00:00\n#SBATCH --job-name=bwa_index_GCF_902167405.1_gadMor3.0_genomic\n#SBATCH --mail-type=FAIL\n#SBATCH --mail-user=sara.schaal@noaa.gov \n#SBATCH --output=../job_outfiles/bwa-index_GCF_902167405.1_gadMor3.0_genomic.%j.out\n#SBATCH --error=../job_outfiles/bwa-index_GCF_902167405.1_gadMor3.0_genomic.%j.err\n\nmodule unload aligners/bwa/0.7.17\nmodule load aligners/bwa/0.7.17\n\nbwa index -p /home/sschaal/pcod/20220125/novaseq/bwa/GCF_902167405.1_gadMor3.0_genomic /home/sschaal/ref_genomes/GCF_902167405.1_gadMor3.0_genomic.fna\n\nIn this example, 1GB of RAM was allocated for 1 hour. The stdout and stderr went to our job_outfiles directory and was given the jobID in the file name. If the job failed and email would be sent to the given email. In order to run the index, the program bwa was needed and therefore, needed to be loaded via a module. As a refresher, you can get the name of the module and the path to the program in that module using the module avail command. The last line is bwa index script for indexing the reference genome.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>SEDNA and Slurm</span>"
    ]
  },
  {
    "objectID": "02-Text_Editors.html",
    "href": "02-Text_Editors.html",
    "title": "3  Text Editors",
    "section": "",
    "text": "3.0.1 Local text editors vs. unix text editors\nTo write your scripts, you can either write via a local text editor and then upload the script to the cluster or you can write/edit a script directly on the command line. It is good practice to always update your scripts on both your local drive copy and on Sedna in order to keep track of changes you have made. Whether you write/edit locally or on sedna is your personal preference, but ensure you keep an updated copy on both locations.\n\n3.0.1.1 Sublime Text\nIN PROGRESS\n\n\n3.0.1.2 vim\nIN PROGRESS\n\n\n\n3.0.2 Moving files to and from servers\nTo move a files, you should use the scp command which is generally formatted as follows scp &lt;path from source&gt; &lt;path to destination&gt;\nTo move a file from your local computer to sedna you can use the following script:\n\nscp -P 22 &lt;username&gt;@161.55.52.157:&lt;SEDNA_PATH&gt;/&lt;filename&gt; &lt;LOCAL_PATH&gt;\n\nTo move a file from sedna to your local computer you can use the following:\n\nscp -P 22 &lt;LOCAL_PATH&gt;\\&lt;filename&gt; &lt;username&gt;@sedna.nwfsc2.noaa.gov:&lt;SEDNA_PATH&gt;\n\nTo move an entire directory you would add the -r flag:\n\nscp -P 22 -r &lt;username&gt;@161.55.52.157:&lt;SEDNA_PATH&gt;/&lt;foldername&gt; &lt;LOCAL_PATH&gt;",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Editors</span>"
    ]
  },
  {
    "objectID": "03-Project_Directories.html",
    "href": "03-Project_Directories.html",
    "title": "4  Project Directories",
    "section": "",
    "text": "4.1 Guide for Setting up Project Directory\nBefore beginning a project, you’ll want to set up a directory for your project. This can be named in whatever flavor you want, but an example would be setting a directory with the date you began the project and a few word description (e.g., 20231106_pcod_lcWGS). Alternatively if you work on a number of different projects for a given species and those analyses don’t interact maybe you want a header directory called ‘pcod’ and then within that a subfolder with that dates run (e.g., 20231106_lcWGS).\nIN PROGRESS",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Project Directories</span>"
    ]
  },
  {
    "objectID": "04-sed_awk_grep.html",
    "href": "04-sed_awk_grep.html",
    "title": "5  sed / awk / grep",
    "section": "",
    "text": "5.1 Quick guide to some powerful command line utilities\nIntroductory text IN PROGRESS",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sed / awk / grep</span>"
    ]
  },
  {
    "objectID": "04-sed_awk_grep.html#quick-guide-to-some-powerful-command-line-utilities",
    "href": "04-sed_awk_grep.html#quick-guide-to-some-powerful-command-line-utilities",
    "title": "5  sed / awk / grep",
    "section": "",
    "text": "5.1.1 sed\nIN PROGRESS\n\n\n5.1.2 awk\nIN PROGRESS\n\n\n5.1.3 grep\nIN PROGRESS",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>sed / awk / grep</span>"
    ]
  },
  {
    "objectID": "05-File_Transfers.html",
    "href": "05-File_Transfers.html",
    "title": "6  File Transfer",
    "section": "",
    "text": "6.1 RCLONE\nRclone allows you to transfer information between google drive and sedna. This is the most likely need for rclone in this lab, but it is not limited to that application. The Sedna informational document has basic steps to get started with rclone, which can use to get started in place of this handbook. This page provides instructions on how to began with rclone, but it will not cover all of rclone’s usages, which can be found at their website. However, if you are accessing the remote server on a Windows computer, you may run into more issues than that of Linux/Mac, and troubleshooting steps are included below if necessary.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>File Transfer</span>"
    ]
  },
  {
    "objectID": "05-File_Transfers.html#configuring-rclone-on-your-sedna-account",
    "href": "05-File_Transfers.html#configuring-rclone-on-your-sedna-account",
    "title": "6  File Transfer",
    "section": "6.2 Configuring rclone on your sedna account",
    "text": "6.2 Configuring rclone on your sedna account\nThis section is essentially a copy of what is on the SEDNA bioinformatics cluster information document, but it is also present here to get you started. This information is currently only to transfer from sedna to a google drive account, but rclone has more services than just this.\nWhen you first connect to sedna, use the below command by adding the local host.\n\n# replace USERNAME with your sedna account name\nssh -L localhost:53682:localhost:53682 USERNAME@sedna.nwfsc2.noaa.gov\n\nFirst, load rclone on sedna.\n\nmodule load tools/rclone/1.59.2\n\nConfigure a remote directory using the following steps:\n\n# Step 1\nrclone config\n\n# Step 2\n# type n for \"new remote\" and name it, then hit enter\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; n\nname&gt; RCLONE_DRIVE_NAME\n\n# Step 3\n# choose your storage, which is likely Google Drive\n# if google drive, type 18\nOption Storage.\nType of storage to configure.\nChoose a number from below, or type in your own value.\n 1 / 1Fichier\n   \\ (fichier)\n 2 / Akamai NetStorage\n   \\ (netstorage)\n 3 / Alias for an existing remote\n   \\ (alias)\n 4 / Amazon Drive\n   \\ (amazon cloud drive)\n...\n16 / FTP\n   \\ (ftp)\n17 / Google Cloud Storage (this is not Google Drive)\n   \\ (google cloud storage)\n18 / Google Drive\n   \\ (drive)\n19 / Google Photos\n   \\ (google photos)\n20 / HTTP\n   \\ (http)\n...\n49 / seafile\n   \\ (seafile)\nStorage&gt; 18\n\n# Step 4\n# just hit enter without typing anything here\nOption client_id.\nGoogle Application Client Id\nSetting your own is recommended.\nSee https://rclone.org/drive/#making-your-own-client-id for how to create your own.\nIf you leave this blank, it will use an internal key which is low performance.\nEnter a value. Press Enter to leave empty.\nclient_id&gt;\n\n# Step 5\n# same here, just enter\nOption client_secret.\nOAuth Client Secret.\nLeave blank normally.\nEnter a value. Press Enter to leave empty.\nclient_secret&gt;\n\n#Step 6\n# choose 1 for full access\nOption scope.\nScope that rclone should use when requesting access from drive.\nChoose a number from below, or type in your own value.\nPress Enter to leave empty.\n 1 / Full access all files, excluding Application Data Folder.\n   \\ (drive)\n 2 / Read-only access to file metadata and file contents.\n   \\ (drive.readonly)\n   / Access to files created by rclone only.\n 3 | These are visible in the drive website.\n   | File authorization is revoked when the user deauthorizes the app.\n   \\ (drive.file)\n   / Allows read and write access to the Application Data folder.\n 4 | This is not visible in the drive website.\n   \\ (drive.appfolder)\n   / Allows read-only access to file metadata but\n 5 | does not allow any access to read or download file content.\n   \\ (drive.metadata.readonly)\nscope&gt; 1\n\n# Step 7\n# again, hit enter\nOption service_account_file.\nService Account Credentials JSON file path.\nLeave blank normally.\nNeeded only if you want use SA instead of interactive login.\nLeading `~` will be expanded in the file name as will environment variables such as `${RCLONE_CONFIG_DIR}`.\nEnter a value. Press Enter to leave empty.\nservice_account_file&gt;\n\n# Step 8\n# do not edit advanced contig, n\nEdit advanced config?\ny) Yes\nn) No (default)\ny/n&gt; n\n\n# Step 9\n# This step is where you may run into issues with a PC\n# First, try \"y\" for auto config\nUse auto config?\n * Say Y if not sure\n * Say N if you are working on a remote or headless machine\ny) Yes (default)\nn) No\ny/n&gt; y\n\n# this will provide the following output, which contains a website that you will copy and paste into a browser on your computer\n2024/02/08 12:04:56 NOTICE: If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=r9jyi5-A0or3ILWoMU6WqQ\n2024/02/08 12:04:56 NOTICE: Log in and authorize rclone for access\n2024/02/08 12:04:56 NOTICE: Waiting for code...\n\nIf the auto config works, it will give you this screen, to which you would select “Allow”:\n\nThis will result in a new screen that says “Success! All done. Please go back to rclone.”\nIf this step does not work as shown above (often the website will show an error), skip to the “Troubleshooting” section below.\nIf the above step worked, you should see “Got code” on your ssh, and the next step will appear.\n\n2024/02/08 12:07:06 NOTICE: Got code\n\n# Step 10\n# Choose whichever matches your destination\n# If transferring to/from ABL Genetics, select y\n# If transferring to/from your own drive, select n\nConfigure this as a Shared Drive (Team Drive)?\n\ny) Yes\nn) No (default)\ny/n&gt; y\n\n# Step 11\n# select ABL Genetics, 1\nOption config_team_drive.\nShared Drive\nChoose a number from below, or type in your own string value.\nPress Enter for the default (0AFAMZunTPxmIUk9PVA).\n 1 / ABL Genetics\n   \\ (0AFAMZunTPxmIUk9PVA)\nconfig_team_drive&gt; 1\n\n# Step 12\n# Results, and say \"Yes this is OK\", y\nConfiguration complete.\nOptions:\n- type: drive\n- scope: drive\n- token: {\"access_token\":\"ya29.a0AfB_byDuRDuRvJDxOmj-HHz6bWinV4FRRzDYmXFj9MriIjURlXFGvTftorRwBXcAOmxUy22xjrVtDSfgMP4BoUU6v4Bcc6r3PSBZsMbNR_k5bGBccC0cipjfAZsK0x9_Rj11-6c0ihQTLC3kj3eXmiHqUbojaCNhIpSpaCgYKARoSARESFQHGX2MiXF3vjKG4BidUr_isGHQQ8A0171\",\"token_type\":\"Bearer\",\"refresh_token\":\"1//06BWwBypNxLDJCgYIARAAGAYSNwF-L9Ir1jygSebphM0ZicoL24k5P_vsQ5rrrnmqSlCVHCZ-b8J2jHUXhI1ZTX30FeK6yVEL90M\",\"expiry\":\"2024-02-08T13:07:05.909470766-08:00\"}\n- team_drive: 0AFAMZunTPxmIUk9PVA\n- root_folder_id:\nKeep this \"test\" remote?\ny) Yes this is OK (default)\ne) Edit this remote\nd) Delete this remote\ny/e/d&gt; y\n\n# Step 13\n# Lastly, it will show you your current configured remotes\n# Quit to get out of rclone config, q\n# To set up another remote directory, n\nCurrent remotes:\n\nName                 Type\n====                 ====\nablg_shared          drive\nnh_google            drive\nRCLONE_DRIVE_NAME    drive\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q\n\nYou may notice that there are three directories set up in this example. It may be easiest to create two rclone remote directories immediately - one for the shared ABL Genetics Google drive and another for your separate Google drive folders. If you first set it up for the shared drive, you can go through the above steps again and create a new remote directory for your own drive folders by selecting “No” on the Shared Drive step.\nCheck that your remote directory leads you to the Google Drive folders you want:\n\n# list directories\nrclone lsd RCLONE_DRIVE_NAME:\n  \n# for the ABL Genetics folder, I named my rclone directory \"ablg_shared\"\nrclone lsd ablg_shared:\n          -1 2023-10-30 14:18:15        -1 Whole genome backup\n          -1 2023-11-20 11:59:28        -1 rockfishLET\n\nIf the data to be transferred is shared with you on Google (which is different from a shared drive), use the --drive-shared-with-me flag. You can only access this if you do not set up your directory as a shared drive.\n\nrclone lsd RCLONE_DRIVE_NAME: --drive-shared-with-me\n          -1 2021-04-22 17:00:11        -1 SEDNA for users\n          -1 2023-11-19 22:49:14        -1 pink_salmon_run_timing\n          -1 2023-11-20 10:11:41        -1 sockeye_whole_genome_dat_for_laura\n\n# investigate subfolders\nrclone lsd RCLONE_DRIVE_NAME:pink_salmon_run_timing --drive-shared-with-me\n          -1 2023-12-06 15:19:45        -1 Data\n          -1 2023-11-20 10:31:36        -1 HighRes_Figures\n\nYou can search within subdirectories. Also, if you want to see the content of the folders, use ls to list all the files and the file sizes.\n\nrclone ls RCLONE_DRIVE_NAME:pink_salmon_run_timing/Data --drive-shared-with-me\n      108 Pink_blocklist_1x.txt\n    22080 Pink_filtered_bamslist.txt\n\nThere are a host of other rclone commands, and you can peruse the “commands” tab on their website to see further functionality. For example, rclone tree can be used to look at the files, and all files in subdirectories.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>File Transfer</span>"
    ]
  },
  {
    "objectID": "05-File_Transfers.html#transferring-data-tofrom-your-rclone-directory",
    "href": "05-File_Transfers.html#transferring-data-tofrom-your-rclone-directory",
    "title": "6  File Transfer",
    "section": "6.3 Transferring data to/from your rclone directory",
    "text": "6.3 Transferring data to/from your rclone directory\nFirst, get on a compute node for the data transfer.\n\nsrun -c 1 -t 12:00:00 --pty /bin/bash\n\nUse the rclone copy command to transfer data. It incorporates its own checksum system during the transfer, so you can cancel and restart a transfer, and it should pick up where it left off without issues. If you want to see output during the transfer, you can add the verbose flag -v at the end of the line. If you are transferring to/from a drive shared with you, make sure to include the --drive-shared-with-me flag.\n\n# FROM SEDNA TO GOOGLE DRIVE\nrclone copy /home/usr/directory/to/file.txt RCLONE_DRIVE_NAME:google/drive/directory -v\n\n# From Google Drive to sedna for a google folder that was shared with me\nrclone copy RCLONE_DRIVE_NAME:google/drive/directory /home/usr/directory/to/transfer --drive-shared-with-me\n\nIf you plan to transfer entire folders, you can do more than one file at a time.\n\n# navigate to directory \ncd /directory/to/transfer\n\nrclone copy . RCLONE_DRIVE_NAME:google/drive/directory/ -v\n\n# alternatively, you can copy one file at a time using the following for loop (from Laura), which can also be used to subset the data to transfer this way as well.\nfor i in /directory/to/transfer/ABLG*; do echo ${i}; rclone copy ${i} RCLONE_DRIVE_NAME:google/drive/directory/ -v; done\n\nCheck if the entire folder was transferred properly if you are currently in the directory that has transferred. NH - need to double check the code for this (does it accept the astericks?).\n\nrclone check RCLONE_DRIVE_NAME:google_drive_folder/* . --one-way\n\nDrawback of rclone: If you are transferring large amounts of data, you may run into an issue where your user rate limit is exceeded. This is a function of Google security measures that prevent continuous transferring in order to ward off illegal activities. As far as I am aware, there is no immediate solution, so you may have to wait hours (or longer) to restart your data transfer.\n\nERROR : Failed to copy: googleapi: Error 403: User rate limit exceeded., userRateLimitExceeded",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>File Transfer</span>"
    ]
  },
  {
    "objectID": "05-File_Transfers.html#troubleshooting-rclone-config---likely-just-for-pc-users.",
    "href": "05-File_Transfers.html#troubleshooting-rclone-config---likely-just-for-pc-users.",
    "title": "6  File Transfer",
    "section": "6.4 Troubleshooting rclone config - likely just for PC users.",
    "text": "6.4 Troubleshooting rclone config - likely just for PC users.\nIf the auto config does not work, you may have to download rclone onto your home computer. Start by downloading rclone: https://downloads.rclone.org/v1.59.2/\nExtract the executable file.\nOn the command line for your home computer, navigate to the folder where you extracted the executable file. For me that is:\n\ncd C:\\rclone\\rclone-v1.52.9-windows-amd64\n\n# I only had to complete this step once after downloading.\nstart .\\rclone.exe \n\nNow return to rclone config on sedna. You may have to delete the failed remote directory and start a new one, completing all the steps as above until you get to use auto config?, for which you say no.\nReturn to your home computer’s command line and type the following to get the token verification:\n\n.\\rclone authorize \"drive\"\n\n# Which results in a similar output to below:\n\nIf your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth?state=JF1IBQPMBWThTru98O2MKw Log in and authorize rclone for access Waiting for code... Got code Paste the following into your remote machine ---\\&gt; {\"access_token\":\"ya29.a0AfB_byAxDN2Pf5cBYrBcK9P8_hhgVSfztkf_K_iQ2j15febzaFqlSNPsJXfj9L4b2J7rXXIFzbq7e6W-mrupSU--p0tTi7iA6rnpdwC95mYQk_h7gU6m56Xa-LFu32WJyencRkz2lt1d_MS8-wN1jDN4OI9Y8k1H-8xJaCgYKAfASARESFQHGX2MibtA9KFMk-266SPqNRR_Zow0171\",\"token_type\":\"Bearer\",\"refresh_token\":\"1//06RybmGeNK7-kCgYIARAAGAYSNwF-L9Ir2qWdHgTpfdzNMLDUhPSr7_keDi6edSn9_T61ADm06_o7lmV0U6uSFKJBlpwgrunn6hA\",\"expiry\":\"2023-11-20T21:21:50.0984646-08:00\"} \\&lt;---End paste\n\nCopy everything including the {} and paste in the sedna ssh rclone config token verification. It should pop up a window and then you should be able to continue with the configuration as described above.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>File Transfer</span>"
    ]
  }
]